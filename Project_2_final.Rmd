---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(rjson)
library(dplyr)
library(tokenizers)
library(stopwords)
library(stringr)
library(tidyverse)

jobs_2020 = fromJSON(file = "indeed_job_descs_2020_09_20.json")
jobs_2021 = fromJSON(file = "indeed_job_descs_2021_01_25.json")
```


## Getting the data into a workable DF
```{r}
jobs_df = data.frame("Job", "State", "Employment", 
                     "description", 2020)

names(jobs_df) = c("Job","State","Employment", "Description", "Year")

for(i in 1:length(jobs_2020)) {
  job = jobs_2020[[i]]$request_params[1]
  state = jobs_2020[[i]]$request_params[2]
  employment = jobs_2020[[i]]$request_params[3]
  year = 2020
  
  #print(i)
  
  for(j in 1:length(jobs_2020[[i]]$job_descriptions)) {
    descript = jobs_2020[[i]]$job_descriptions[j]
    temp_df = as.data.frame(c(job,state,employment, descript, year))
    names(temp_df) = c("Job","State","Employment", "Description", "Year")
    jobs_df = rbind(jobs_df, temp_df)
    #print(j)
  }
}


head(jobs_df)
dim(jobs_df)

#unique(factor(jobs_df$State))
```
## This is adding the second json file in 
## There is no data for anything past the 4th description
```{r}
#1:4 because every job after the 4th one has no description
for(i in 1:4) {
  job = jobs_2021[[i]]$request_params[1]
  state = jobs_2021[[i]]$request_params[2]
  employment = jobs_2021[[i]]$request_params[3]
  year = 2021
  
  print(i)
  
  for(j in 1:length(jobs_2021[[i]]$job_descriptions)) {
    descript = jobs_2021[[i]]$job_descriptions[j]
    temp_df = as.data.frame(c(job,state,employment, descript, year))
    names(temp_df) = c("Job","State","Employment", "Description", "Year")
    jobs_df = rbind(jobs_df, temp_df)
    print(j)
  }
}

#jobs_2021
head(jobs_df)
dim(jobs_df)

#remove the first row because it is useless
jobs_df <- jobs_df[-1,]

#make the names easier for when we split
jobs_df$Job = str_replace_all(jobs_df$Job, "\\+", "_")

#remove potential sources of weird string behavior
#in a couple of samples it was relatively common to see "a.T" as in the end of one sentence into the next
jobs_df$Description = str_replace_all(jobs_df$Description, "\\.", " ")
```


```{r}
#just some visualization of the data 
par(mar=c(11,4,4,4))
barplot(height = table(jobs_df$Job), las = 2, cex.names = .75)

table(jobs_df$Job)
```
## Tokenize the DF function
You will need to download tokenizers and stopwords
```{r}
#splitting the DF
split_df = split(jobs_df, jobs_df$Job)

#tokenize function that takes job descriptions and tokenizes them in a new DF
#That DF gets merged back with the old one 
tokenize.df = function(df) {
  
  temp = data.frame()
  for (i in 1:length(df$Description)) {
    tokens = table(tokenize_words(df$Description[i], 
                                  stopwords = stopwords::stopwords("en")))
    
    tokens = tokens / length(tokens)
    #temp DF to merge later 
    temp = bind_rows(temp, tokens)
    
  }
  #setting NA to 0 because it is easier to use in this case
  temp[is.na(temp)] = 0
  df = bind_cols(df,temp)
  return(df)
}
```


## Tokenize the DF
```{r}
#for loop to go through each job type and tokenize the description
for (i in 1:length(split_df)) {
  temp = split_df[[i]]
  split_df[[i]] = tokenize.df(temp)
  print(i)
}

```

Getting a Data Frame that only has the tokens 
```{r}
split_df_freqs = split_df
for( i in 1:length(split_df_freqs)) {
  split_df_freqs[[i]] = split_df_freqs[[i]][,6:length(split_df_freqs[[i]])]
}

#getting all tha values to be numeric
for(i in 1:length(split_df_freqs)) {
  for(j in 1:length(split_df_freqs[[i]]))
    split_df_freqs[[i]][,j] = as.numeric(split_df_freqs[[i]][,j])
}

```
Getting the TF_IDF
```{r}
#getting the inverse document frequency numbers 
#takes in a DF that has been split into sub categories 
tf_idf = function(split_df, large_scale = T) {
  #initializing a DF 
  all_tots_df = data.frame()
  send_back = data.frame()
  
  if(large_scale == T){
    #going through all the different jobs 
    for (i in 1:length(split_df)) {
      
      #getting a temp holder
      temp = apply(split_df[[i]][,6:length(split_df[[i]])], 2, sum)
      #getting some sort of normalized data for the master list
      all_tots_df = bind_rows(all_tots_df, temp) 
      
    }
    all_tots_df[is.na(all_tots_df)] = 0
    #first get total colums with a value
    #then take the log factor of value/total docs
    all_tots_df = bind_rows(all_tots_df, colSums(all_tots_df > 0))
    all_tots_df = bind_rows(all_tots_df, log10((nrow(all_tots_df)-1) / all_tots_df[nrow(all_tots_df),]))
    
    #now returning the tf-idf
    inverse = all_tots_df[nrow(all_tots_df),]
    for (i in 1:length(split_df)) {
      freqs = all_tots_df[i,]
      tf_idf = freqs * inverse
      send_back = bind_rows(send_back, tf_idf)
    }
    rownames(send_back) = names(split_df)
    return(round(send_back, 5))
  }
  else {
    #getting totals of the DF
    split_df = bind_rows(split_df, colSums(split_df > 0))
    split_df = bind_rows(split_df, log10((nrow(split_df-1 ) / split_df[nrow(split_df),])))
      
    inverse = split_df[nrow(split_df),]
    for (i in 1:(nrow(split_df)-2)) {
      freqs = split_df[i,]
      tf_idf = split_df[nrow(split_df),] * split_df[i,]
      split_df[i,] = tf_idf
    }
  }
    return(split_df)
}



#tf_idf_df = tf_idf(split_df)
#sort(tf_idf_df[3,], decreasing = T)


for (i in 1:length(split_df_freqs)) {
  print(i)
  split_df_freqs[[i]] = tf_idf(split_df_freqs[[i]], F)
}

#split_df_test = bind_rows(split_df, colSums(split_df_freqs[[1]] > 0))
```







```{r}
tokenize_resume = function(path) {
  #inputting sarah resume 
  resume = readLines(path, warn = F)
  resume = str_c(resume, sep = " ", collapse = " ") 
  
  #tokenizing the resume 
  tokens = table(tokenize_words(resume, 
                                    stopwords = stopwords::stopwords("en")))
  
  tokens = as.list(tokens, all.names = F)
  
  tokens = as.data.frame(tokens)
  return(tokens)
}
zac = tokenize_resume("test.txt")
zac
```

```{r}
#function that multiplys columns in DFs together 
multi = function(df) {
  if (length(df) < 3) {
    return(df[1] * df[2])
  } else {
    times = df[1] * df[2]
    for(i in 3:length(df)) {
      times = df[i] * times  
    }
    return(times)
  }
}

#function that gives best matches to the overall job fields 
#naive/stupid way of doing it 
best_matches_overview = function(li, resume) {
  matches = matrix(NA, nrow = length(li), ncol = 1)
  
  for(i in 1:length(li)) {
    tester = bind_rows(li[[i]], resume)
    tester[is.na(tester)] = 0
    score = sum(apply(tester, 2, multi), na.rm = T)
    #matches[[names(li)[i]]] = score
    matches[i,1] = score
  }
  rownames(matches) = names(li)
  return(matches)
}

sarah = tokenize_resume("sarahresumetxt.txt")
sarah

zac = tokenize_resume("test.txt")
zac

bm_new = best_matches_overview(master_li, zac)
print(sort(bm_new[,1], decreasing = T))

bm_idf = best_matches_overview(idf_list, zac)
print(sort(bm_idf[,1], decreasing = T))
#converting job names to a matrix
job_names  = names(sort(bm[,1], decreasing = T)[1:3])
job_names = str_split_fixed(job_names, "", 1)

```
Since it is a human in the model algo we can then either determine if these seem like they will fit or not.
We could just grab the first 3 and get a few more examples 

```{r}
best_matches = function(df, job_name, resume) {
  matches = list()
  #outerloop for the overall job category
  for (i in 1:nrow(job_name)) {
    #initialize a score and an index to return later on 
    score = 0
    index = 1
    print(job_name[i,1])
    #innerloop to get the best job match from the category
    for(j in 1:nrow(df[[i]])) {
      #temp is a DF of just the jth row only containing columns with tokenization
      temp = df[[job_name[i,1] ]][j, 6:ncol(df[[job_name[i,1]]])]
      tester = bind_rows(temp, resume)
      tester[is.na(tester)] = 0
      new_score = sum(apply(tester, 2, multi), na.rm = T)
      #changes the best seen score 
      if (new_score > score) {
        index = j 
        score = new_score 
      }
      
    }
    #sets the list with name of overall job and appends the best scoring description 
    matches[[job_name[i,1]]] = df[[job_name[i,1]]][index, 4]
    }
  return(matches)
}


best = best_matches(freq_split_df, job_names, zac)
best
```
```{r}
max(master_li$human_resource_specialist)
```

 
```{r}
#split dataframe into new dataframes based on job description
library(dplyr)
master_df <- jobs_df %>%
  group_by(Job)

group_split(master_df)
group_keys(master_df)

#stopwords::stopwords("en", source = "snowball")

```

## NEW METHODS:
### breaaking jobs down to stem and non-stem then comparing jobs

```{r}
master_li_nonfactored = list()
for (i in 1:length(split_df)) {
  
  #getting a temp holder
  temp = apply(split_df[[i]][,6:length(split_df[[i]])], 2, sum)
  #getting a normalization factor 
  #factor = mean(apply(split_df[[i]][,6:length(split_df[[i]])], 1, sum))
  #getting some sort of normalized data for the master list
  master_li_nonfactored[[names(split_df)[i]]] = temp #/ factor
  
}


#stem jobs for a new DF
stem_jobs <- c("ux_designer", "test_engineer", "site_reliability_engineer", "data_architect", "data_scientist", "software_developer", "statistician", "deep_learning", "machine_learning_engineer", "business_analyst")

#non-stem jobs for a new DF
non_stem_jobs <- c("recruiter", "marketing", "sales", "office_manager", "human_resource_specialist", "researcher")

#initializing stem DF
stem_df <- data.frame()

#creating new stem DF 
for (title in stem_jobs) {
  stem_df <- bind_rows(stem_df, master_li_nonfactored[[title]])
  
}
#removing NA and replacing with 0
stem_df[is.na(stem_df)] = 0
stem_df

#initializing 
non_stem_df <- data.frame()

#creating non stem DF
for (title in non_stem_jobs) {
  non_stem_df <- bind_rows(non_stem_df, master_li_nonfactored[[title]])
  
}
#adding 0 for NA
non_stem_df[is.na(non_stem_df)] = 0
non_stem_df
```
```{r}
stem_tokens <- colnames(stem_df)
non_stem_tokens <- colnames(non_stem_df)

#getting unique set of tokens 
unique_stem_tokens <- setdiff(stem_tokens, non_stem_tokens)
unique_non_stem_tokens <- setdiff(non_stem_tokens, stem_tokens)
```


## part for seeing if the resume matches stem or non stem more 

```{r}
stem_df = stem_df[unique_stem_tokens]
non_stem_df = non_stem_df[unique_non_stem_tokens]

#   
#total_stem = sort(apply(stem_df, 2, sum), decreasing = T)
#total_non_stem = sort(apply(non_stem_df, 2, sum), decreasing = T)


#seeing if a resume is stem or non stem 
stem_or_non = function(resume, stem, non_stem){
  count_stem = 0 
  count_non_stem = 0
  for (i in names(resume)) {
    if (i %in% names(stem)) {
      count_stem = count_stem + 1 
    }
    else if (i %in% names(non_stem)) {
      count_non_stem = count_stem + 1
    }
  }
  li = list("stem" = count_stem, "non_stem" = count_non_stem)
  return(li)
}



values = stem_or_non(zac, stem_df, non_stem_df)
```



```{r}

#create a var to represent weather the resume is stem or not
if (values$stem < values$non_stem) {
  resume_is_stem = FALSE
} else {
  resume_is_stem = T
}


#new df to hold the randomly sampled job descriptions
#these job descs will be used to mine parameters to fit a predictive model that gives what the user will like
sample_job_descriptions <- data.frame()

#non stem df
non_stem_jobs_df <- jobs_df[jobs_df$Job=="recruiter"|jobs_df$Job=="marketing"|jobs_df$Job=="sales"|                      jobs_df$Job=="office_manager"|jobs_df$Job=="human_resource_specialist" |jobs_df$Job=="researcher" ,]

#stem df
stem_jobs_df <- jobs_df[jobs_df$Job == "ux_designer" | jobs_df$Job == "test_engineer" |jobs_df$Job == "site_reliability_engineer" |jobs_df$Job == "data_architect" |jobs_df$Job == "data_scientist" |jobs_df$Job == "software_developer" |jobs_df$Job == "statistician" |jobs_df$Job == "deep_learning" |jobs_df$Job == "machine_learning_engineer" |jobs_df$Job == "business_analyst",]

sampling = function(resume_is_stem, N) {
  #if the resume is flagged as stem
  if(resume_is_stem == FALSE){
    
    #randomly sample n jobs from the master df
    sample_job_descriptions <- sample_n(non_stem_jobs_df, N)
    
    #create new col to represent if the user liked the job or not
    sample_job_descriptions$liked_or_not <- NA
    
    #loop thru the df, show the user a job desc and ask them if they like it. If they like it, assign a value of 1 to the liked_or_not col, if they    don't assign a value of 0
    for (index in 1:N) {
      job_desc <- str(sample_job_descriptions$Description[index])
      user_input <- readline(prompt= job_desc)
      if(user_input == "y"){
        sample_job_descriptions$liked_or_not[index] = 1
      }else{
        sample_job_descriptions$liked_or_not[index] = 0
      }
    } 
    #repeat process for stem
  }else{
    sample_job_descriptions <- sample_n(stem_jobs_df, N)
    sample_job_descriptions$liked_or_not <- NA
    for (index in 1:N) {
      job_desc <- str(sample_job_descriptions$Description[index])
      user_input <- readline(prompt= job_desc)
      if(user_input == "y"){
        sample_job_descriptions$liked_or_not[index] = 1
        print(sample_job_descriptions$liked_or_not[index])
      }else{
        sample_job_descriptions$liked_or_not[index] = 0
      }
    }
  }
  return(sample_job_descriptions)
}


#making a training set 
train = sampling(resume_is_stem, 10)
train = tokenize.df(train)

#need df with 2 cols, 1 is yes/no, and the other is a token column that lists the token


```








```{r}
#getting test jobs
test_jobs <- sample_n(non_stem_jobs_df, 5)
test_jobs = tokenize.df(test_jobs)

#splitting into Y~X
y_train = train[,6]
x_train = train[,7:length(train)]

x_test = test_jobs[, 6:length(test_jobs)] 

x_train = as.data.frame(as.matrix(x_train))

for(i in names(x_train)) {
  print(i)
  x_train[i,] = as.integer(x_train[i,])
}

name = apply(x_train,2,sum) > 5
x_train = x_train[,name]


mod <- glm(y_train ~ . , data = x_train,   family=binomial(link="logit"))
test_probs <- predict(mod, newdata=x_test, type="response")



mod <- glm(y_train ~ . , data = x_train,   family=binomial(link="logit"))

class(y_train)
library(glmnet)

lasso_cv <- cv.glmnet(x_train, train$liked_or_not, alpha=1)
coef(lasso_cv)
lasso_mod <- glmnet(x_train, y_train, alpha=1, lambda=lasso_cv$lambda.1se)
coef(lasso_mod)




cos_sim <- function(v1, v2){
  numerator <- sum(v1*v2)
  denominator <- sqrt(sum(v1^2)) * sqrt(sum(v2^2))
  output <- numerator / denominator
  return(output)
}
```



```{r}
library(tm)
library(dplyr)
#take a sample of the job descriptions to minimize runtime
sample_desc <- sample_n(jobs_df, 20)

final_df <- jobs_df

#read in the resume (I used zach's)
resume_string <- read_file("sarahresumetxt.txt")

#create a new with the resume info
resume_df <- data.frame("resume", "NY", "fulltime", resume_string, "2021")
names(resume_df) <- colnames(sample_desc)

#add the resume to the sample descriptions
sample_desc <- rbind(sample_desc, resume_df)

final_df <- rbind(final_df, resume_df)

#create a new df formatted in the way needed to make a corpus object to work w tm library
descs <- data.frame(
  doc_id <- seq(1:nrow(final_df)),
  text <- final_df$Description
)
names(descs) <- c("doc_id", "text")


#object that allows the data frame to be turned into a corpus
ds <- DataframeSource(descs)

#create a corpus object
#corpus objects are objects that are used to process all the different texts
corpus <- Corpus(ds)

#preprocessing text
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument, language="english")

#creating term matrix with TF-IDF weighting
tdm <-DocumentTermMatrix(corpus, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
mat <- as.matrix(tdm)

#function to turn it into cosine similiarity matrix
cos_sim = function(matrix){
    numerator = matrix %*% t(matrix)
    A = sqrt(apply(matrix^2, 1, sum))
    denumerator = A %*% t(A)
    return(numerator / denumerator)
}


cos_sim_matrix <- cos_sim(mat)

#3 best matching resumes are dataframe rows 485, 503, 522
#3 worst matches are df rows 757, 384, 80

#you figure this out ^ by just sorting the matrix for row/column 760
#I added the resume as the last row in the df, so the cos sim from the last row is what we are looking for
#then you take the row/col numbers of those rows and use them as the row # for the jobs_df dataframe and you will get the job descriptions


#Best recommendations, gives indexes that can be inputted into jobs_df to retrieve descriptions

best_recs <- as.numeric(names(sort(cos_sim_matrix[nrow(cos_sim_matrix),], decreasing = TRUE)[2:4]))



hist(cos_sim_matrix, main = "Cosine Similarity Scores in the Matrix", xlab = "Cosine Similarity")

```

```{r}
descs_indexes <- as.numeric(names(sort(cos_sim_matrix[760,], decreasing = TRUE)[2:4])) 


subset_df <- jobs_df
diversify <- function(vector){
  for(index in vector){
  subset_df <- subset(subset_df, Job != jobs_df[index,]$Job)
  }
  
  
  final_df <- subset_df
  final_df <- rbind(final_df, resume_df)

  #create a new df formatted in the way needed to make a corpus object to work w tm library
descs <- data.frame(
  doc_id <- seq(1:nrow(final_df)),
  text <- final_df$Description
)
names(descs) <- c("doc_id", "text")


#object that allows the data frame to be turned into a corpus
ds <- DataframeSource(descs)

#create a corpus object
#corpus objects are objects that are used to process all the different texts
corpus <- Corpus(ds)

#preprocessing text
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument, language="english")

#creating term matrix with TF-IDF weighting
tdm <-DocumentTermMatrix(corpus, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
mat <- as.matrix(tdm)

cos_sim_matrix <- cos_sim(mat)


diverse_recs <- as.numeric(names(sort(cos_sim_matrix[nrow(cos_sim_matrix),], decreasing = TRUE)[2:4]))

for(i in best_recs){
  print(jobs_df[i,]$Description)
}

return(cos_sim_matrix, diverse_recs)



}

diverse_recs <- diversify(descs_indexes)


```

```{r}

```




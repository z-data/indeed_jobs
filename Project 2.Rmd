---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(rjson)
library(dplyr)
library(tokenizers)
library(stopwords)
library(stringr)

jobs_2020 = fromJSON(file = "indeed_job_descs_2020_09_20.json")
jobs_2021 = fromJSON(file = "indeed_job_descs_2021_01_25.json")
```


Getting the data into a workable DF
```{r}
jobs_df = data.frame("Job", "State", "Employment", 
                     "description", 2020)

names(jobs_df) = c("Job","State","Employment", "Description", "Year")

for(i in 1:length(jobs_2020)) {
  job = jobs_2020[[i]]$request_params[1]
  state = jobs_2020[[i]]$request_params[2]
  employment = jobs_2020[[i]]$request_params[3]
  year = 2020
  
  #print(i)
  
  for(j in 1:length(jobs_2020[[i]]$job_descriptions)) {
    descript = jobs_2020[[i]]$job_descriptions[j]
    temp_df = as.data.frame(c(job,state,employment, descript, year))
    names(temp_df) = c("Job","State","Employment", "Description", "Year")
    jobs_df = rbind(jobs_df, temp_df)
    #print(j)
  }
}


head(jobs_df)
dim(jobs_df)

#unique(factor(jobs_df$State))
```
There is no data for anything past the 4th description
```{r}
#1:4 because every job after the 4th one has no description
for(i in 1:4) {
  job = jobs_2021[[i]]$request_params[1]
  state = jobs_2021[[i]]$request_params[2]
  employment = jobs_2021[[i]]$request_params[3]
  year = 2021
  
  print(i)
  
  for(j in 1:length(jobs_2021[[i]]$job_descriptions)) {
    descript = jobs_2021[[i]]$job_descriptions[j]
    temp_df = as.data.frame(c(job,state,employment, descript, year))
    names(temp_df) = c("Job","State","Employment", "Description", "Year")
    jobs_df = rbind(jobs_df, temp_df)
    print(j)
  }
}

#jobs_2021
head(jobs_df)
dim(jobs_df)

#remove the first row because it is useless
jobs_df <- jobs_df[-1,]

#make the names easier for when we split
jobs_df$Job = str_replace_all(jobs_df$Job, "\\+", "_")

#remove potential sources of weird string behavior
#in a couple of samples it was relatively common to see "a.T" as in the end of one sentence into the next
jobs_df$Description = str_replace_all(jobs_df$Description, "\\.", " ")
```


```{r}
#just some visualization of the data 
par(mar=c(11,4,4,4))
barplot(height = table(jobs_df$Job), las = 2, cex.names = .75)

table(jobs_df$Job)
```

You will need to download tokenizers and stopwords
```{r}
#splitting the DF
split_df = split(jobs_df, jobs_df$Job)

#tokenize function that takes job descriptions and tokenizes them in a new DF
#That DF gets merged back with the old one 
tokenize.df = function(df) {
  
  temp = data.frame()
  for (i in 1:length(df$Description)) {
    tokens = table(tokenize_words(df$Description[i], 
                                  stopwords = stopwords::stopwords("en")))
    #temp DF to merge later 
    temp = bind_rows(temp, tokens)
    
  }
  #setting NA to 0 because it is easier to use in this case
  temp[is.na(temp)] = 0
  df = bind_cols(df,temp)
  return(df)
}
```


```{r}
tokenize_ngrams.df = function(df) {
  
  temp = data.frame()
  for (i in 1:length(df$Description)) {
    tokens = table(tokenize_ngrams(df$Description[i], n = 3, n_min = 1,
                                  stopwords = stopwords::stopwords("en")))
    #temp DF to merge later 
    temp = bind_rows(temp, tokens)
    
  }
  #setting NA to 0 because it is easier to use in this case
  temp[is.na(temp)] = 0
  df = bind_cols(df,temp)
  return(df)
}
```


This part takes some time
```{r}

#for loop to go through each job type and tokenize the description
for (i in 1:length(split_df)) {
  temp = split_df[i]
  split_df[[i]] = tokenize.df(temp[[1]])
  print(i)
}

```
This part is grabbing the totals of each column 
```{r}
master_li = list()
for (i in 1:length(split_df)) {
  
  #getting a temp holder
  temp = apply(split_df[[i]][,6:length(split_df[[i]])], 2, sum)
  #getting a normalization factor 
  factor = mean(apply(split_df[[i]][,6:length(split_df[[i]])], 1, sum))
  #getting some sort of normalized data for the master list
  master_li[[names(split_df)[i]]] = temp / factor
  
}

max(master_li$human_resource_specialist)
```


```{r}

#getting the inverse document frequency numbers 
tf_idf = function(split_df) {
  #initializing a DF 
  all_tots_df = data.frame()
  send_back = data.frame()
  
  #going through all the different jobs 
  for (i in 1:length(split_df)) {
    
    #getting a temp holder
    temp = apply(split_df[[i]][,6:length(split_df[[i]])], 2, sum)
    #getting some sort of normalized data for the master list
    all_tots_df = bind_rows(all_tots_df, temp / length(temp)) 
    
  }
  all_tots_df[is.na(all_tots_df)] = 0
  #first get total colums with a value
  #then take the log factor of value/total docs
  all_tots_df = bind_rows(all_tots_df, colSums(all_tots_df > 0))
  all_tots_df = bind_rows(all_tots_df, log10((nrow(all_tots_df)-1) / all_tots_df[nrow(all_tots_df),]))
  
  #now returning the tf-idf
  inverse = all_tots_df[nrow(all_tots_df),]
  print(length(inverse))
  for (i in 1:length(split_df)) {
    freqs = all_tots_df[i,]
    print(length(freqs))
    tf_idf = freqs * inverse
    send_back = bind_rows(send_back, tf_idf)
  }
  rownames(send_back) = names(split_df)
  return(round(send_back, 5))
}

tf_idf_df = tf_idf(split_df)
sort(tf_idf_df[3,], decreasing = T)

apply(tf_idf_df,1,function(x) sort(x, decreasing = T))

tf_idf_df = tf_idf_df[name]


```







```{r}
tokenize_resume = function(path) {
  #inputting sarah resume 
  resume = readLines(path, warn = F)
  resume = str_c(resume, sep = " ", collapse = " ") 
  
  #tokenizing the resume 
  tokens = table(tokenize_words(resume, 
                                    stopwords = stopwords::stopwords("en")))
  
  tokens = as.list(tokens, all.names = F)
  
  tokens = as.data.frame(tokens)
  return(tokens)
}
zac = tokenize_resume("test.txt")
zac
```

```{r}
#function that multiplys columns in DFs together 
multi = function(df) {
  if (length(df) < 3) {
    return(df[1] * df[2])
  } else {
    times = df[1] * df[2]
    for(i in 3:length(df)) {
      times = df[i] * times  
    }
    return(times)
  }
}

#function that gives best matches to the overall job fields 
#naive/stupid way of doing it 
best_matches_overview = function(li, resume) {
  matches = matrix(NA, nrow = length(li), ncol = 1)
  
  for(i in 1:length(li)) {
    tester = bind_rows(li[[i]], resume)
    tester[is.na(tester)] = 0
    score = sum(apply(tester, 2, multi), na.rm = T)
    #matches[[names(li)[i]]] = score
    matches[i,1] = score
  }
  rownames(matches) = names(li)
  return(matches)
}

sarah = tokenize_resume("sarahresumetxt.txt")
sarah

zac = tokenize_resume("test.txt")
zac

bm = best_matches_overview(master_li, sarah)
print(sort(bm[,1], decreasing = T))

#converting job names to a matrix
job_names  = names(sort(bm[,1], decreasing = T)[1:3])
job_names = str_split_fixed(job_names, "", 1)

```
Since it is a human in the model algo we can then either determine if these seem like they will fit or not.
We could just grab the first 3 and get a few more examples 

```{r}
best_matches = function(df, job_name, resume) {
  matches = list()
  #outerloop for the overall job category
  for (i in 1:nrow(job_name)) {
    #initialize a score and an index to return later on 
    score = 0
    index = 1
    print(job_name[i,1])
    #innerloop to get the best job match from the category
    for(j in 1:nrow(df[[i]])) {
      #temp is a DF of just the jth row only containing columns with tokenization
      temp = df[[job_name[i,1] ]][j, 6:ncol(df[[job_name[i,1]]])]
      tester = bind_rows(temp, resume)
      tester[is.na(tester)] = 0
      new_score = sum(apply(tester, 2, multi), na.rm = T)
      #changes the best seen score 
      if (new_score > score) {
        index = j 
        score = new_score 
      }
      
    }
    #sets the list with name of overall job and appends the best scoring description 
    matches[[job_name[i,1]]] = df[[job_name[i,1]]][index, 4]
    }
  return(matches)
}


best = best_matches(split_df, job_names, sarah)
best
```
```{r}
max(master_li$human_resource_specialist)
```

 
```{r}
#split dataframe into new dataframes based on job description
library(dplyr)
master_df <- jobs_df %>%
  group_by(Job)

group_split(master_df)
group_keys(master_df)

#stopwords::stopwords("en", source = "snowball")

```

## NEW METHODS:
### breaaking jobs down to stem and non-stem then comparing jobs

```{r}
master_li_nonfactored = list()
for (i in 1:length(split_df)) {
  
  #getting a temp holder
  temp = apply(split_df[[i]][,6:length(split_df[[i]])], 2, sum)
  #getting a normalization factor 
  #factor = mean(apply(split_df[[i]][,6:length(split_df[[i]])], 1, sum))
  #getting some sort of normalized data for the master list
  master_li_nonfactored[[names(split_df)[i]]] = temp #/ factor
  
}


#stem jobs for a new DF
stem_jobs <- c("ux_designer", "test_engineer", "site_reliability_engineer", "data_architect", "data_scientist", "software_developer", "statistician", "deep_learning", "machine_learning_engineer", "business_analyst")

#non-stem jobs for a new DF
non_stem_jobs <- c("recruiter", "marketing", "sales", "office_manager", "human_resource_specialist", "researcher")

#initializing stem DF
stem_df <- data.frame()

#creating new stem DF 
for (title in stem_jobs) {
  stem_df <- bind_rows(stem_df, master_li_nonfactored[[title]])
  
}
#removing NA and replacing with 0
stem_df[is.na(stem_df)] = 0
stem_df

#initializing 
non_stem_df <- data.frame()

#creating non stem DF
for (title in non_stem_jobs) {
  non_stem_df <- bind_rows(non_stem_df, master_li_nonfactored[[title]])
  
}
#adding 0 for NA
non_stem_df[is.na(non_stem_df)] = 0
non_stem_df
```
```{r}
stem_tokens <- colnames(stem_df)
non_stem_tokens <- colnames(non_stem_df)

#getting unique set of tokens 
unique_stem_tokens <- setdiff(stem_tokens, non_stem_tokens)
unique_non_stem_tokens <- setdiff(non_stem_tokens, stem_tokens)
```


## part for seeing if the resume matches stem or non stem more 

```{r}
stem_df = stem_df[unique_stem_tokens]
non_stem_df = non_stem_df[unique_non_stem_tokens]

#   
#total_stem = sort(apply(stem_df, 2, sum), decreasing = T)
#total_non_stem = sort(apply(non_stem_df, 2, sum), decreasing = T)


#seeing if a resume is stem or non stem 
stem_or_non = function(resume, stem, non_stem){
  count_stem = 0 
  count_non_stem = 0
  for (i in names(resume)) {
    if (i %in% names(stem)) {
      count_stem = count_stem + 1 
    }
    else if (i %in% names(non_stem)) {
      count_non_stem = count_stem + 1
    }
  }
  li = list("stem" = count_stem, "non_stem" = count_non_stem)
  return(li)
}



values = stem_or_non(zac, stem_df, non_stem_df)
```



```{r}

#create a var to represent weather the resume is stem or not
if (values$stem < values$non_stem) {
  resume_is_stem = FALSE
} else {
  resume_is_stem = T
}


#new df to hold the randomly sampled job descriptions
#these job descs will be used to mine parameters to fit a predictive model that gives what the user will like
sample_job_descriptions <- data.frame()

#non stem df
non_stem_jobs_df <- jobs_df[jobs_df$Job=="recruiter"|jobs_df$Job=="marketing"|jobs_df$Job=="sales"|                      jobs_df$Job=="office_manager"|jobs_df$Job=="human_resource_specialist" |jobs_df$Job=="researcher" ,]

#stem df
stem_jobs_df <- jobs_df[jobs_df$Job == "ux_designer" | jobs_df$Job == "test_engineer" |jobs_df$Job == "site_reliability_engineer" |jobs_df$Job == "data_architect" |jobs_df$Job == "data_scientist" |jobs_df$Job == "software_developer" |jobs_df$Job == "statistician" |jobs_df$Job == "deep_learning" |jobs_df$Job == "machine_learning_engineer" |jobs_df$Job == "business_analyst",]

sampling = function(resume_is_stem, N) {
  #if the resume is flagged as stem
  if(resume_is_stem == FALSE){
    
    #randomly sample n jobs from the master df
    sample_job_descriptions <- sample_n(non_stem_jobs_df, N)
    
    #create new col to represent if the user liked the job or not
    sample_job_descriptions$liked_or_not <- NA
    
    #loop thru the df, show the user a job desc and ask them if they like it. If they like it, assign a value of 1 to the liked_or_not col, if they    don't assign a value of 0
    for (index in 1:N) {
      job_desc <- str(sample_job_descriptions$Description[index])
      user_input <- readline(prompt= job_desc)
      if(user_input == "y"){
        sample_job_descriptions$liked_or_not[index] = 1
      }else{
        sample_job_descriptions$liked_or_not[index] = 0
      }
    } 
    #repeat process for stem
  }else{
    sample_job_descriptions <- sample_n(stem_jobs_df, N)
    sample_job_descriptions$liked_or_not <- NA
    for (index in 1:N) {
      job_desc <- str(sample_job_descriptions$Description[index])
      user_input <- readline(prompt= job_desc)
      if(user_input == "y"){
        sample_job_descriptions$liked_or_not[index] = 1
        print(sample_job_descriptions$liked_or_not[index])
      }else{
        sample_job_descriptions$liked_or_not[index] = 0
      }
    }
  }
  return(sample_job_descriptions)
}


#making a training set 
train = sampling(resume_is_stem, 10)
train = tokenize.df(train)

#need df with 2 cols, 1 is yes/no, and the other is a token column that lists the token


```








```{r}
#getting test jobs
test_jobs <- sample_n(non_stem_jobs_df, 5)
test_jobs = tokenize.df(test_jobs)

#splitting into Y~X
y_train = train[,6]
x_train = train[,7:length(train)]

x_test = test_jobs[, 6:length(test_jobs)] 

x_train = as.data.frame(as.matrix(x_train))

for(i in names(x_train)) {
  print(i)
  x_train[i,] = as.integer(x_train[i,])
}

name = apply(x_train,2,sum) > 5
x_train = x_train[,name]


mod <- glm(y_train ~ . , data = x_train,   family=binomial(link="logit"))
test_probs <- predict(mod, newdata=x_test, type="response")



mod <- glm(y_train ~ . , data = x_train,   family=binomial(link="logit"))

class(y_train)
library(glmnet)

lasso_cv <- cv.glmnet(x_train, train$liked_or_not, alpha=1)
coef(lasso_cv)
lasso_mod <- glmnet(x_train, y_train, alpha=1, lambda=lasso_cv$lambda.1se)
coef(lasso_mod)




cos_sim <- function(v1, v2){
  numerator <- sum(v1*v2)
  denominator <- sqrt(sum(v1^2)) * sqrt(sum(v2^2))
  output <- numerator / denominator
  return(output)
}
```


```{r}
master_li


tf_idf = function(df) {}
  
  
  
  
}
```










